/**
 * Î∞∞Ïπò Î¨∏Ï†ú Ïú†Ìòï Î∂ÑÎ•ò Ïä§ÌÅ¨Î¶ΩÌä∏
 *
 * grammar_file / literature_file ÌÖåÏù¥Î∏îÏóêÏÑú HWP Î∞îÏù¥ÎÑàÎ¶¨Î•º ÏùΩÍ≥†,
 * ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú ‚Üí Î¨∏Ï†ú ÌååÏã± ‚Üí Ïú†Ìòï Î∂ÑÎ•ò ‚Üí parsed_question ÌÖåÏù¥Î∏îÏóê INSERT
 *
 * ÏÇ¨Ïö©Î≤ï:
 *   npx tsx scripts/batch-classify-questions.ts [options]
 *
 * ÏòµÏÖò:
 *   --dry-run         DBÏóê Ï†ÄÏû•ÌïòÏßÄ ÏïäÍ≥† Í≤∞Í≥ºÎßå Ï∂úÎ†•
 *   --type grammar    Î¨∏Î≤ïÎßå Ï≤òÎ¶¨ (Í∏∞Î≥∏: Îëò Îã§)
 *   --type literature  Î¨∏ÌïôÎßå Ï≤òÎ¶¨
 *   --limit 10        Ï≤òÎ¶¨Ìï† ÏµúÎåÄ Î©îÌÉÄ Í±¥Ïàò
 *   --meta-id 123     ÌäπÏ†ï Î©îÌÉÄ IDÎßå Ï≤òÎ¶¨
 */

import * as dotenv from 'dotenv';
import * as path from 'path';

// .env Î°úÎìú (apps/root/.env)
dotenv.config({ path: path.resolve(__dirname, '../apps/root/.env') });

import mysql from 'mysql2/promise';
import type { Pool, RowDataPacket } from 'mysql2/promise';
import { extractTextFromHwp } from '../apps/root/src/lib/hwp-extractor';
import { parseHwpQuestions, type ParsedQuestion } from './parse-questions';

// ‚îÄ‚îÄ CLI Ïù∏Ïûê ÌååÏã± ‚îÄ‚îÄ

const args = process.argv.slice(2);
function getArg(name: string): string | undefined {
  const idx = args.indexOf(`--${name}`);
  return idx >= 0 && idx + 1 < args.length ? args[idx + 1] : undefined;
}
const DRY_RUN = args.includes('--dry-run');
const PROCESS_TYPE = getArg('type') as 'grammar' | 'literature' | undefined;
const LIMIT = getArg('limit') ? parseInt(getArg('limit')!, 10) : undefined;
const META_ID = getArg('meta-id') ? parseInt(getArg('meta-id')!, 10) : undefined;
const BATCH_SIZE = 500;

// ‚îÄ‚îÄ DB Ïó∞Í≤∞ ‚îÄ‚îÄ

function createPool(): Pool {
  return mysql.createPool({
    host: process.env.DB_HOST,
    port: Number(process.env.DB_PORT) || 3306,
    database: process.env.DB_NAME || 'kaca',
    user: process.env.DB_USER,
    password: process.env.DB_PASSWORD,
    charset: 'utf8mb4',
    waitForConnections: true,
    connectionLimit: 5,
  });
}

// ‚îÄ‚îÄ Î©îÏù∏ ‚îÄ‚îÄ

interface FileMeta {
  metaId: number;
  fileId: number;
  fileName: string;
  content: Buffer;
  category: string;      // grammar_meta.sub_category ÎòêÎäî literature_meta.category
}

async function fetchGrammarFiles(pool: Pool): Promise<FileMeta[]> {
  let sql = `
    SELECT gm.id AS metaId, gf.id AS fileId, gf.file_name AS fileName,
           gf.content, gm.sub_category AS category
    FROM grammar_meta gm
    JOIN grammar_file gf ON gf.meta_id = gm.id
    WHERE gf.content IS NOT NULL
      AND gf.file_name LIKE '%.hwp'
  `;
  const params: unknown[] = [];

  if (META_ID) {
    sql += ` AND gm.id = ${Number(META_ID)}`;
  }

  // Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Í±¥ Ï†úÏô∏
  sql += ` AND NOT EXISTS (
    SELECT 1 FROM parsed_question pq
    WHERE pq.source_type = 'grammar' AND pq.source_meta_id = gm.id
  )`;

  sql += ' ORDER BY gm.id';
  if (LIMIT) {
    sql += ` LIMIT ${Number(LIMIT)}`;
  }

  // LONGBLOB + prepared statement LIMIT Î≤ÑÍ∑∏ ÌöåÌîº: query() ÏÇ¨Ïö©
  const [rows] = await pool.query<RowDataPacket[]>(sql, params);
  return rows.map(r => ({
    metaId: r.metaId,
    fileId: r.fileId,
    fileName: r.fileName,
    content: r.content as Buffer,
    category: r.category || 'Î¨∏Î≤ï',
  }));
}

async function fetchLiteratureFiles(pool: Pool): Promise<FileMeta[]> {
  let sql = `
    SELECT lm.id AS metaId, lf.id AS fileId, lf.file_name AS fileName,
           lf.content, lm.category
    FROM literature_meta lm
    JOIN literature_file lf ON lf.meta_id = lm.id
    WHERE lf.content IS NOT NULL
      AND lf.file_name LIKE '%.hwp'
  `;

  if (META_ID) {
    sql += ` AND lm.id = ${Number(META_ID)}`;
  }

  sql += ` AND NOT EXISTS (
    SELECT 1 FROM parsed_question pq
    WHERE pq.source_type = 'literature' AND pq.source_meta_id = lm.id
  )`;

  sql += ' ORDER BY lm.id';
  if (LIMIT) {
    sql += ` LIMIT ${Number(LIMIT)}`;
  }

  const [rows] = await pool.query<RowDataPacket[]>(sql);
  return rows.map(r => ({
    metaId: r.metaId,
    fileId: r.fileId,
    fileName: r.fileName,
    content: r.content as Buffer,
    category: r.category || 'Î¨∏Ìïô',
  }));
}

async function insertBatch(
  pool: Pool,
  sourceType: 'grammar' | 'literature',
  metaId: number,
  questions: ParsedQuestion[]
): Promise<number> {
  if (questions.length === 0) return 0;

  const values = questions.map(q => [
    sourceType,
    metaId,
    q.questionNumber,
    q.questionText,
    JSON.stringify(q.choices),
    q.answer,
    q.questionType.code,
    q.questionType.confidence,
  ]);

  const placeholders = values.map(() => '(?, ?, ?, ?, ?, ?, ?, ?)').join(', ');
  const flat = values.flat();

  const [result] = await pool.execute(
    `INSERT INTO parsed_question
      (source_type, source_meta_id, question_number, question_text, choices, answer, question_type, confidence)
     VALUES ${placeholders}`,
    flat
  );

  return (result as any).affectedRows || 0;
}

function mapCategoryForClassifier(sourceType: string, category: string): string | undefined {
  if (sourceType === 'grammar') return 'Î¨∏Î≤ï';
  // literature_meta.categoryÎäî 'Î¨∏Ìïô'Ïù¥ Í∏∞Î≥∏
  if (category === 'Î¨∏Ìïô' || category === 'ÎèÖÏÑú' || category === 'ÌôîÏûë') return category;
  return 'Î¨∏Ìïô';
}

async function processFiles(
  pool: Pool,
  files: FileMeta[],
  sourceType: 'grammar' | 'literature'
): Promise<{ total: number; parsed: number; failed: number }> {
  let totalQuestions = 0;
  let failedFiles = 0;

  for (let i = 0; i < files.length; i++) {
    const file = files[i];
    const progress = `[${i + 1}/${files.length}]`;

    try {
      // HWP ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
      const text = extractTextFromHwp(file.content);
      if (!text || text.trim().length < 20) {
        console.log(`${progress} ‚è≠ meta=${file.metaId} (${file.fileName}) - ÌÖçÏä§Ìä∏ ÏóÜÏùå`);
        continue;
      }

      // Î¨∏Ï†ú ÌååÏã±
      const classifyCategory = mapCategoryForClassifier(sourceType, file.category);
      const questions = parseHwpQuestions(text, classifyCategory);

      if (questions.length === 0) {
        console.log(`${progress} ‚è≠ meta=${file.metaId} (${file.fileName}) - Î¨∏Ï†ú 0Í±¥`);
        continue;
      }

      if (DRY_RUN) {
        console.log(`${progress} üìã meta=${file.metaId} (${file.fileName}) - ${questions.length}Í±¥`);
        for (const q of questions) {
          const typeLabel = `${q.questionType.code} (${q.questionType.nameKo})`;
          const conf = (q.questionType.confidence * 100).toFixed(0);
          console.log(`  Q${q.questionNumber}: ${typeLabel} [${conf}%] ${q.questionText.substring(0, 60)}...`);
        }
      } else {
        // DB INSERT (BATCH_SIZE Îã®ÏúÑ)
        for (let j = 0; j < questions.length; j += BATCH_SIZE) {
          const batch = questions.slice(j, j + BATCH_SIZE);
          await insertBatch(pool, sourceType, file.metaId, batch);
        }
        console.log(`${progress} ‚úÖ meta=${file.metaId} - ${questions.length}Í±¥ Ï†ÄÏû•`);
      }

      totalQuestions += questions.length;
    } catch (err) {
      failedFiles++;
      const errMsg = err instanceof Error ? err.message : String(err);
      console.error(`${progress} ‚ùå meta=${file.metaId} (${file.fileName}) - ${errMsg}`);
    }
  }

  return { total: files.length, parsed: totalQuestions, failed: failedFiles };
}

async function main() {
  console.log('=== Î¨∏Ï†ú Ïú†Ìòï Î∞∞Ïπò Î∂ÑÎ•ò ===');
  console.log(`Î™®Îìú: ${DRY_RUN ? 'DRY-RUN (Ï†ÄÏû•ÌïòÏßÄ ÏïäÏùå)' : 'LIVE'}`);
  if (PROCESS_TYPE) console.log(`Ïú†Ìòï: ${PROCESS_TYPE}`);
  if (LIMIT) console.log(`Ï†úÌïú: ${LIMIT}Í±¥`);
  if (META_ID) console.log(`Î©îÌÉÄ ID: ${META_ID}`);
  console.log('');

  const pool = createPool();

  try {
    // grammar Ï≤òÎ¶¨
    if (!PROCESS_TYPE || PROCESS_TYPE === 'grammar') {
      console.log('--- Î¨∏Î≤ï ÌååÏùº Ï≤òÎ¶¨ ---');
      const grammarFiles = await fetchGrammarFiles(pool);
      console.log(`ÎåÄÏÉÅ: ${grammarFiles.length}Í±¥\n`);

      if (grammarFiles.length > 0) {
        const result = await processFiles(pool, grammarFiles, 'grammar');
        console.log(`\nÎ¨∏Î≤ï ÏôÑÎ£å: ${result.parsed}Î¨∏Ï†ú ÌååÏã±, ${result.failed}Í±¥ Ïã§Ìå®\n`);
      }
    }

    // literature Ï≤òÎ¶¨
    if (!PROCESS_TYPE || PROCESS_TYPE === 'literature') {
      console.log('--- Î¨∏Ìïô ÌååÏùº Ï≤òÎ¶¨ ---');
      const litFiles = await fetchLiteratureFiles(pool);
      console.log(`ÎåÄÏÉÅ: ${litFiles.length}Í±¥\n`);

      if (litFiles.length > 0) {
        const result = await processFiles(pool, litFiles, 'literature');
        console.log(`\nÎ¨∏Ìïô ÏôÑÎ£å: ${result.parsed}Î¨∏Ï†ú ÌååÏã±, ${result.failed}Í±¥ Ïã§Ìå®\n`);
      }
    }

    console.log('=== ÏôÑÎ£å ===');
  } finally {
    await pool.end();
  }
}

main().catch(err => {
  console.error('ÏπòÎ™ÖÏ†Å Ïò§Î•ò:', err);
  process.exit(1);
});
